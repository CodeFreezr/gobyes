lapply(load_packages, require, character.only = TRUE)
load_packages=c("janeaustenr","tidytext","dplyr","stringr","ggplot2","wordcloud","reshape2","igraph","ggraph","widyr","tidyr")
lapply(load_packages, require, character.only = TRUE)
Install.packages("MDPtoolbox")
library(tensorflow)
pip3 install --upgrade pip
pip3 install upgrade pip
pip3 install upgrade pip
pip3 install jupyter
library(text2vec)
library(dplyr)
library(tensorflow)
library(RCurl)
library(tidytext)
# Read movie dataset
data("movie_review")
labels <- as.matrix(data.frame("Positive_flag" = movie_review$sentiment,
"negative_flag" = (1-movie_review$sentiment)))
reviews <- data.frame("Sno" = 1:nrow(movie_review),
"text"=movie_review$review,
stringsAsFactors=F)
# Encoding the words
reviews_words <- reviews %>% unnest_tokens(word,text)
reviews_sortedWords <- reviews %>% unnest_tokens(word,text) %>% dplyr::count(word, sort = TRUE)
reviews_sortedWords$orderNo <- 1:nrow(reviews_sortedWords)
reviews_sortedWords <- as.data.frame(reviews_sortedWords)
reviews_words <- plyr::join(reviews_words,reviews_sortedWords,by="word")
reviews_words_sno <- list()
for(i in 1:length(reviews$text)){
reviews_words_sno[[i]] <- c(subset(reviews_words,Sno==i,orderNo))
if(i %% 500 == 0) cat("completed : ", i,"\n")
}
# Distribution for unique number of words across all reviews
wordLen_counter <- data.frame("counts"=unlist(lapply(reviews_words_sno,function(x) length(x[[1]])))) %>% dplyr::count(counts)
hist(x=wordLen_counter$counts,
freq=wordLen_counter$n)
# Curtail the number of words in each review to 150
reviews_words_sno <- lapply(reviews_words_sno,function(x) {
x <- x$orderNo
if(length(x)>150){
return (x[1:150])
} else {
return(c(rep(0,150-length(x)),x))
}
})
# create train/validation/test datasets
train_samples <- caret::createDataPartition(c(1:length(labels[,1])),p = 0.7)$Resample1
train_reviews <- reviews_words_sno[train_samples]
test_reviews <- reviews_words_sno[-train_samples]
train_reviews <- do.call(rbind,train_reviews)
test_reviews <- do.call(rbind,test_reviews)
train_labels <- as.matrix(labels[train_samples,])
test_labels <- as.matrix(labels[-train_samples,])
# Reset the graph and start interactive session
tf$reset_default_graph()
sess<-tf$InteractiveSession()
# Model parameters
n_input<-15
step_size<-10
n.hidden<-2
n.class<-2
# Training Parameters
lr<-0.01
batch<-200
iteration = 500
# Function to evaluate mean accuracy
eval_acc<-function(yhat, y){
# Count correct solution
correct_Count = tf$equal(tf$argmax(yhat,1L), tf$argmax(y,1L))
# Mean accuracy
mean_accuracy = tf$reduce_mean(tf$cast(correct_Count, tf$float32))
return(mean_accuracy)
}
with(tf$name_scope('input'), {
# Define placeholder for input data
x = tf$placeholder(tf$float32, shape=shape(NULL, step_size, n_input), name='x')
y <- tf$placeholder(tf$float32, shape(NULL, n.class), name='y')
# Define Weights and bias
weights <- tf$Variable(tf$random_normal(shape(n.hidden, n.class)))
bias <- tf$Variable(tf$random_normal(shape(n.class)))
})
lstm<-function(x, weight, bias){
# Unstack input into step_size
x = tf$unstack(x, step_size, 1)
# Define a lstm cell
lstm_cell = tf$contrib$rnn$BasicLSTMCell(n.hidden, forget_bias=1.0, state_is_tuple=TRUE)
# Get lstm cell output
cell_output = tf$contrib$rnn$static_rnn(lstm_cell, x, dtype=tf$float32)
# Linear activation, using rnn inner loop last output
last_vec=tail(cell_output[[1]], n=1)[[1]]
return(tf$matmul(last_vec, weights) + bias)
}
# Evaluate rnn cell output
yhat = lstm(x, weights, bias)
cost = tf$reduce_mean(tf$nn$softmax_cross_entropy_with_logits(logits=yhat, labels=y))
optimizer = tf$train$AdamOptimizer(learning_rate=lr)$minimize(cost)
sess$run(tf$global_variables_initializer())
train_error <- c()
for(i in 1:iteration){
spls <- sample(1:dim(train_reviews)[1],batch)
sample_data<-train_reviews[spls,]
sample_y<-train_labels[spls,]
# Reshape sample into 15 sequence with each of 10 elements
sample_data=tf$reshape(sample_data, shape(batch, step_size, n_input))
out<-optimizer$run(feed_dict = dict(x=sample_data$eval(session = sess), y=sample_y))
if (i %% 1 == 0){
cat("iteration - ", i, "Training Loss - ",  cost$eval(feed_dict = dict(x=sample_data$eval(), y=sample_y)), "\n")
}
train_error <-  c(train_error,cost$eval(feed_dict = dict(x=sample_data$eval(), y=sample_y)))
}
# Plot training error reduction
plot(train_error, main="Training sentiment prediction error", xlab="Iterations", ylab = "Train Error")
test_data=tf$reshape(test_reviews, shape(-1, step_size, n_input))
cost$eval(feed_dict=dict(x=test_data$eval(), y=test_labels))
library(text2vec)
library(dplyr)
library(tensorflow)
library(RCurl)
library(tidytext)
# Read movie dataset
data("movie_review")
labels <- as.matrix(data.frame("Positive_flag" = movie_review$sentiment,
"negative_flag" = (1-movie_review$sentiment)))
reviews <- data.frame("Sno" = 1:nrow(movie_review),
"text"=movie_review$review,
stringsAsFactors=F)
# Encoding the words
reviews_words <- reviews %>% unnest_tokens(word,text)
reviews_sortedWords <- reviews %>% unnest_tokens(word,text) %>% dplyr::count(word, sort = TRUE)
reviews_sortedWords$orderNo <- 1:nrow(reviews_sortedWords)
reviews_sortedWords <- as.data.frame(reviews_sortedWords)
reviews_words <- plyr::join(reviews_words,reviews_sortedWords,by="word")
reviews_words_sno <- list()
for(i in 1:length(reviews$text)){
reviews_words_sno[[i]] <- c(subset(reviews_words,Sno==i,orderNo))
if(i %% 500 == 0) cat("completed : ", i,"\n")
}
# Distribution for unique number of words across all reviews
wordLen_counter <- data.frame("counts"=unlist(lapply(reviews_words_sno,function(x) length(x[[1]])))) %>% dplyr::count(counts)
hist(x=wordLen_counter$counts,
freq=wordLen_counter$n)
# Curtail the number of words in each review to 150
reviews_words_sno <- lapply(reviews_words_sno,function(x) {
x <- x$orderNo
if(length(x)>150){
return (x[1:150])
} else {
return(c(rep(0,150-length(x)),x))
}
})
# create train/validation/test datasets
train_samples <- caret::createDataPartition(c(1:length(labels[,1])),p = 0.7)$Resample1
train_reviews <- reviews_words_sno[train_samples]
test_reviews <- reviews_words_sno[-train_samples]
train_reviews <- do.call(rbind,train_reviews)
test_reviews <- do.call(rbind,test_reviews)
train_labels <- as.matrix(labels[train_samples,])
test_labels <- as.matrix(labels[-train_samples,])
# Reset the graph and start interactive session
tf$reset_default_graph()
sess<-tf$InteractiveSession()
# Model parameters
n_input<-15
step_size<-10
n.hidden<-2
n.class<-2
# Training Parameters
lr<-0.01
batch<-200
iteration = 500
# Function to evaluate mean accuracy
eval_acc<-function(yhat, y){
# Count correct solution
correct_Count = tf$equal(tf$argmax(yhat,1L), tf$argmax(y,1L))
# Mean accuracy
mean_accuracy = tf$reduce_mean(tf$cast(correct_Count, tf$float32))
return(mean_accuracy)
}
with(tf$name_scope('input'), {
# Define placeholder for input data
x = tf$placeholder(tf$float32, shape=shape(NULL, step_size, n_input), name='x')
y <- tf$placeholder(tf$float32, shape(NULL, n.class), name='y')
# Define Weights and bias
weights <- tf$Variable(tf$random_normal(shape(n.hidden, n.class)))
bias <- tf$Variable(tf$random_normal(shape(n.class)))
})
lstm<-function(x, weight, bias){
# Unstack input into step_size
x = tf$unstack(x, step_size, 1)
# Define a lstm cell
lstm_cell = tf$contrib$rnn$BasicLSTMCell(n.hidden, forget_bias=1.0, state_is_tuple=TRUE)
# Get lstm cell output
cell_output = tf$contrib$rnn$static_rnn(lstm_cell, x, dtype=tf$float32)
# Linear activation, using rnn inner loop last output
last_vec=tail(cell_output[[1]], n=1)[[1]]
return(tf$matmul(last_vec, weights) + bias)
}
# Evaluate rnn cell output
yhat = lstm(x, weights, bias)
cost = tf$reduce_mean(tf$nn$softmax_cross_entropy_with_logits(logits=yhat, labels=y))
optimizer = tf$train$AdamOptimizer(learning_rate=lr)$minimize(cost)
sess$run(tf$global_variables_initializer())
train_error <- c()
for(i in 1:iteration){
spls <- sample(1:dim(train_reviews)[1],batch)
sample_data<-train_reviews[spls,]
sample_y<-train_labels[spls,]
# Reshape sample into 15 sequence with each of 10 elements
sample_data=tf$reshape(sample_data, shape(batch, step_size, n_input))
out<-optimizer$run(feed_dict = dict(x=sample_data$eval(session = sess), y=sample_y))
if (i %% 1 == 0){
cat("iteration - ", i, "Training Loss - ",  cost$eval(feed_dict = dict(x=sample_data$eval(), y=sample_y)), "\n")
}
train_error <-  c(train_error,cost$eval(feed_dict = dict(x=sample_data$eval(), y=sample_y)))
}
# Plot training error reduction
plot(train_error, main="Training sentiment prediction error", xlab="Iterations", ylab = "Train Error")
test_data=tf$reshape(test_reviews, shape(-1, step_size, n_input))
cost$eval(feed_dict=dict(x=test_data$eval(), y=test_labels))
auto <- read.csv("auto-mpg.csv", header=TRUE, sep = ",")
auto <- read.csv("auto-mpg.csv", header=TRUE)
auto-mpg.csv
> auto <- read.csv("auto-mpg.csv", header=TRUE, sep = ",")
auto <- read.csv("auto-mpg.csv", header=TRUE, sep = ",")
auto <- read.csv("auto-mpg.csv", header=TRUE, sep = ",")
auto <- read.csv("auto-mpg.csv", header=TRUE, sep = ",")
> auto <- read.csv("auto-mpg.csv", header=TRUE, sep = ",")
> auto <- read.csv("auto-mpg.csv", header=TRUE)
> auto <- read.csv("auto-mpg.csv", header=TRUE)
> auto <- read.csv("auto-mpg.csv", header=TRUE, sep = ",")
auto <- read.csv("auto-mpg.csv", header=TRUE, sep = ",")
> train <- read.csv("train.csv", header = TRUE)
train <- read.csv("train.csv", header = TRUE)
train <- read.csv("train.csv", header = TRUE)
test <- read.csv("test.csv", header = TRUE)
test.survived <- data.frame(survived = rep("None", nrow(test)), test[,])
library(ggplot2)
library("caret", lib.loc="~/R/R-3.2.5/library")
library("assertthat", lib.loc="~/R/R-3.2.5/library")
library("yaml", lib.loc="~/R/R-3.2.5/library")
library("xml2", lib.loc="~/R/R-3.2.5/library")
library("withr", lib.loc="~/R/R-3.2.5/library")
library("whisker", lib.loc="~/R/R-3.2.5/library")
library("uuid", lib.loc="~/R/R-3.2.5/library")
library("utils", lib.loc="~/R/R-3.2.5/library")
library("translations", lib.loc="~/R/R-3.2.5/library")
library("tools", lib.loc="~/R/R-3.2.5/library")
library("tokenizers", lib.loc="~/R/R-3.2.5/library")
library("tidyr", lib.loc="~/R/R-3.2.5/library")
library("tfruns", lib.loc="~/R/R-3.2.5/library")
library("text2vec", lib.loc="~/R/R-3.2.5/library")
library("tcltk", lib.loc="~/R/R-3.2.5/library")
library("testthat", lib.loc="~/R/R-3.2.5/library")
library("survival", lib.loc="~/R/R-3.2.5/library")
library("stringr", lib.loc="~/R/R-3.2.5/library")
library("stringi", lib.loc="~/R/R-3.2.5/library")
library("stringdist", lib.loc="~/R/R-3.2.5/library")
library("stats4", lib.loc="~/R/R-3.2.5/library")
library("stats", lib.loc="~/R/R-3.2.5/library")
library("splines", lib.loc="~/R/R-3.2.5/library")
library("spatial", lib.loc="~/R/R-3.2.5/library")
library("SparseM", lib.loc="~/R/R-3.2.5/library")
library("sp", lib.loc="~/R/R-3.2.5/library")
library("SnowballC", lib.loc="~/R/R-3.2.5/library")
library("scales", lib.loc="~/R/R-3.2.5/library")
library("rversions", lib.loc="~/R/R-3.2.5/library")
library("rstudioapi", lib.loc="~/R/R-3.2.5/library")
library("rprojroot", lib.loc="~/R/R-3.2.5/library")
library("rpart", lib.loc="~/R/R-3.2.5/library")
library("roxygen2", lib.loc="~/R/R-3.2.5/library")
library("rmarkdown", lib.loc="~/R/R-3.2.5/library")
library("rex", lib.loc="~/R/R-3.2.5/library")
library("reticulate", lib.loc="~/R/R-3.2.5/library")
library("reshape2", lib.loc="~/R/R-3.2.5/library")
library("repr", lib.loc="~/R/R-3.2.5/library")
library("readbitmap", lib.loc="~/R/R-3.2.5/library")
library("RCurl", lib.loc="~/R/R-3.2.5/library")
library("RcppParallel", lib.loc="~/R/R-3.2.5/library")
library("RcppEigen", lib.loc="~/R/R-3.2.5/library")
library("Rcpp", lib.loc="~/R/R-3.2.5/library")
library("RColorBrewer", lib.loc="~/R/R-3.2.5/library")
library("R6", lib.loc="~/R/R-3.2.5/library")
library("R.utils", lib.loc="~/R/R-3.2.5/library")
library("R.rsp", lib.loc="~/R/R-3.2.5/library")
train <- read.csv("train.csv", header = TRUE)
install.packages("XML")
library(XML)
url <- "http://www.w3schools.com/xml/cd_catalog.xml"
xmldoc <- xmlParse(url)
url <- "http://www.w3schools.com/xml/cd_catalog.xml"
xmldoc <- xmlParse(url)
rootNode <- xmlRoot(xmldoc)
install.packages("jsonlite")
library(jsonlite)
dat.1 <- fromJSON("students.json")
dat.2 <- fromJSON("student-courses.json")
url <- "http://finance.yahoo.com/webservice/v1/symbols/allcurrencies/quote?format=json"
jsonDoc <- fromJSON(url)
dat <- jsonDoc$list$resources$resource$fields
dat <- jsonDoc$list$resources$resource$fields
dat.1 <- jsonDoc$list$resources$resource$fields
dat.2 <- jsonDoc$list$resources$resource$fields
dat[1:2,]
dat.1[1:3,]
dat.2[,c(1,2,4:5)]
install.packages("XML")
library(XML)
url <- "http://www.w3schools.com/xml/cd_catalog.
url <- "http://www.w3schools.com/xml/cd_catalog.xml"
url <- "http://www.w3schools.com/xml/cd_catalog.xml"
xmldoc <- xmlParse(url)
install.packages("jsonlite")
student  <- read.fwf("student-fwf.txt", widths=c(4,15,20,15,4), col.names=c("id","name","email","major","year"))
student-fwf.txt
student-fwf.txt
install.packages("scales")
library(scales)
students <- read.csv("data-conversion.csv")
install.packages("scales")
library(scales)
install.packages("jsonlite")
student  <- read.fwf("student-fwf.txt", widths=c(4,15,20,15,4), col.names=c("id","name","email","major","year"))
student  <- read.fwf("student/fwf.txt", widths=c(4,15,20,15,4), col.names=c("id","name","email","major","year"))
students <- read.csv("data/conversion.csv")
setwd("C:/Users/sagarsawant\AppData\Local\Temp\RtmpkvtmIX\downloaded_packages")
housing <- read.csv("BostonHousing.csv")
import("BostonHousing.csv")
library("rio")
install.packages("rio")
library("rio")
library(rio)
library("R.utils", lib.loc="~/R/R-3.2.5/library")
library("scales", lib.loc="~/R/R-3.2.5/library")
students <- read.csv("data/conversion.csv")
students <- read.csv("data-conversion.csv")
b <- c(-Inf, 10000, 31000, Inf)
names <- c("Low", "Medium", "High")
students$Income.cat <- cut(students$Income, breaks = b, labels = names)
> students
Age State Gender Height Income Income.cat
1   23    NJ      F     61   5000        Low
2   13    NY      M     55   1000        Low
3   36    NJ      M     66   3000        Low
4   31    VA      F     64   4000        Low
5   58    NY      F     70  30000     Medium
6   29    TX      F     63  10000        Low
7   39    NJ      M     67  50000       High
8   50    VA      M     70  55000       High
9   23    TX      F     61   2000        Low
10  36    VA      M     66  20000     Medium
students$Income.cat <- cut(students$Income, breaks = b, labels = names)
students
Age State Gender Height Income Income.cat
1   23    NJ      F     61   5000        Low
2   13    NY      M     55   1000        Low
3   36    NJ      M     66   3000        Low
4   31    VA      F     64   4000        Low
5   58    NY      F     70  30000     Medium
6   29    TX      F     63  10000        Low
7   39    NJ      M     67  50000       High
8   50    VA      M     70  55000       High
9   23    TX      F     61   2000        Low
10  36    VA      M     66  20000     Medium
install.packages("dummies")
library(dummies)
students <- read.csv("data-conversion.csv")
install.packages("Hmisc")
housing.dat <- read.csv("housing-with-missing-value.csv",header = TRUE, stringsAsFactors = FALSE)
summary(housing.dat)
data()
data(package = .packages(all.available = TRUE))
ave.image(file = "all.RData")
customer <- c("John", "Peter", "Jane")
orderdate <- as.Date(c('2014-10-1','2014-1-2','2014-7-6'))
orderamount <- c(280, 100.50, 40.25)
order <- data.frame(customer,orderdate,orderamount)
names <- c("John", "Joan")
save(order, names, file="test.Rdata")
saveRDS(order,file="order.rds")
remove(order)
load("test.Rdata")
ord <- readRDS("order.rds")
data(iris)
data(list(cars,iris))
data(iris)
data(list(cars,iris))
data(list(iris))
list(iris)
dat <- read.csv("missing-data.csv", na.strings="")
dat.cleaned <- na.omit(dat)
is.na(dat[4,2])
is.na(dat$Income)
dat.income.cleaned <- dat[!is.na(dat$Income),]
dat.income.cleaned <- dat[!is.na(dat$Income),]
nrow(dat.income.cleaned)
complete.cases(dat)
t
salary <- c(20000, 30000, 25000, 40000, 30000, 34000, 30000)
family.size <- c(4,3,2,2,3,4,3)
car <- c("Luxury", "Compact", "Midsize", "Luxury",     "Compact", "Compact", "Compact")
prospect <- data.frame(salary, family.size, car)
prospect.cleaned <- unique(prospect)
nrow(prospect)
nrow(prospect.cleaned)
duplicated(prospect)
prospect[duplicated(prospect), ]
install.packages("dummies")
library(dummies)
install.packages("dummies")
library(dummies)
students <- read.csv("data-conversion.csv")
housing <- read.csv("BostonHousing.csv")
housing.z <- scale(housing)
scale.many <- function(dat, column.nos) {
nms <- names(dat)
for(col in column.nos) {
name <- paste(nms[col],".z", sep = "")
dat[name] <- scale(dat[,col])
}
cat(paste("Scaled ", length(column.nos), " variable(s)\n"))
dat
}
housing <- read.csv("BostonHousing.csv")
housing <- scale.many(housing, c(1,3,5:7))
students <- read.csv("data-conversion.csv")
b <- c(-Inf, 10000, 31000, Inf)
names <- c("Low", "Medium", "High")
students$Income.cat <- cut(students$Income, breaks = b, labels = names)
students
b <- c(-Inf, 10000, 31000, Inf)
students$Income.cat1 <- cut(students$Income, breaks = b)
students
students$Income.cat2 <- cut(students$Income,
breaks = 4, labels = c("Level1", "Level2",
"Level3","Level4"))
install.packages("mice")
library(mice)
housingData <- read.csv("housing-with-missing-value.csv",header = TRUE, stringsAsFactors = FALSE)
ozoneData <- read.csv("ozone.csv", stringsAsFactors=FALSE)
library(readxl)
datasets <- read_excel("C:/Users/sagarsawant/Downloads/datasets.zip")
View(datasets)
library(readr)
dataset <- read_csv(NULL)
View(dataset)
library(readxl)
ozone <- read_excel("C:\\Users\\sagarsawant\\Downloads\\datasets (1).zip\\ozone.csv")
View(ozone)
install.packages(c("modeest","raster","moments"))
library(modeest)
library(raster)
library(moments)
mean(auto$mpg)
auto <- read.csv("auto-mpg.csv", stringsAsFactors=FALSE)
auto[1:3, 8:9]
install.packages("caret")
library(caret)
bh <- read.csv("BostonHousing.csv")
hist(acceleration)
customer <- c("John", "Peter", "Jane")
orddt <- as.Date(c('2014-10-1','2014-1-2','2014-7-6'))
ordamt <- c(280, 100.50, 40.25)
order <- data.frame(customer,orddt,ordamt)
install.packages("RODBC")
install.packages("RJDBC")
install.packages("RMySQL")
setwd("//192.168.0.200/BookDrafts/5151_Machine Learning With Go/Code/Chapter 9/chapter9_code_bundle/building_a_scalable_pipeline/example2")
{
"pipeline": {
"name": "model"
},
"transform": {
"image": "dwhitena/goregtrain:single",
"cmd": [
"/goregtrain",
"-inDir=/pfs/training",
"-outDir=/pfs/out"
]
},
"parallelism_spec": {
"constant": "1"
},
"input": {
"atom": {
"repo": "training",
"glob": "/"
}
}
}
